{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "\n",
    "from collections import Counter\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error \n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "RAND = 10\n",
    "N_FOLDS = 3\n",
    "TIMEOUT = 10800\n",
    "N_TRIALS = 3000\n",
    "\n",
    "backend_path = os.path.abspath('../backend')\n",
    "sys.path.append(backend_path)\n",
    "model_path = os.path.abspath('../models')\n",
    "sys.path.append(model_path)\n",
    "from get_metrics import get_metrics_regression\n",
    "from check_overfitting import check_overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Чтение DataFrame df в файл data/df.csv\n",
    "df = pd.read_csv('../data/df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# основные описательные статистики для числовых признаков\n",
    "df.iloc[:, 1:].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# основные описательные статистики для булевых и категориальных признаков\n",
    "df.describe(include=[\"object\", \"bool\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем LabelEncoder для кодирования категориальных значений\n",
    "le = LabelEncoder()\n",
    "\n",
    "# список категориальных столбцов\n",
    "categorical_cols = df.select_dtypes(include=[object]).columns\n",
    "\n",
    "# кодируем каждый категориальный столбец\n",
    "for col in categorical_cols:\n",
    "    df[col] = le.fit_transform(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# рассчитываем матрицу корреляции\n",
    "corr_matrix = df.corr()\n",
    "\n",
    "# создаем маску\n",
    "mask = np.abs(corr_matrix) < 0.1\n",
    "\n",
    "# указывает размер графика\n",
    "plt.figure(figsize=(18, 10))\n",
    "\n",
    "# создаем тепловую карту матрицы корреляции\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', square=True, fmt='.2f', mask=mask, linewidths=0.5, linecolor='grey')\n",
    "\n",
    "# Показываем график\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# признаки\n",
    "X = df[['milliseconds', 'place', 'status', 'tsunami', 'significance', 'data_type', 'country', 'depth',\n",
    "        'datetime', 'timezone', 'magnitude_bins', 'year', 'month', 'day', 'hour', 'minute', 'second',\n",
    "        # 'magnitude',\n",
    "        # 'longitude',\n",
    "        # 'latitude'\n",
    "        ]]\n",
    "\n",
    "# целевые переменные\n",
    "y = df[[\n",
    "    'magnitude',\n",
    "    'longitude',\n",
    "    'latitude'\n",
    "    ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_permutation_importance(models, X, y, RAND):\n",
    "    common_features_file = '../data/common_features.pkl'\n",
    "    if os.path.exists(common_features_file):\n",
    "        print(\"Используем предвычисленные общие признаки из\", common_features_file)\n",
    "        with open(common_features_file, 'rb') as f:\n",
    "            common_features = pickle.load(f)\n",
    "        return common_features\n",
    "    else:\n",
    "        print(\"Вычисляем важность признаков с помощью permutation importance...\")\n",
    "        important_features_models = []\n",
    "        for model in models:\n",
    "            importances_result = permutation_importance(model, X, y, n_repeats=3, random_state=RAND)\n",
    "            importances = importances_result.importances_mean\n",
    "            \n",
    "            # сортируем признаки по важности\n",
    "            sorted_importances = sorted(zip(X.columns, importances), key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            # выводим важность признаков для каждой модели\n",
    "            print(f\"Модель {model}:\")\n",
    "            important_features_model = [feature for feature, importance in sorted_importances if importance >= 0.05]\n",
    "            print(important_features_model)\n",
    "            print()\n",
    "            \n",
    "            important_features_models.append(important_features_model)\n",
    "        \n",
    "        # находим общие признаки в каждой модели\n",
    "        feature_counts = Counter(feature for model_features in important_features_models for feature in model_features)\n",
    "        common_features = [feature for feature, count in feature_counts.items() if count == len(important_features_models)]\n",
    "        \n",
    "        with open(common_features_file, 'wb') as f:\n",
    "            pickle.dump(common_features, f)\n",
    "        \n",
    "        print(\"Общие важные признаки для моделей:\")\n",
    "        print(common_features)\n",
    "        return common_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем список моделей\n",
    "models = [\n",
    "    RandomForestRegressor(random_state=RAND),\n",
    "    MultiOutputRegressor(GradientBoostingRegressor(random_state=RAND)),\n",
    "    DecisionTreeRegressor(random_state=RAND),\n",
    "    MultiOutputRegressor(LGBMRegressor(random_state=RAND, verbosity=-1)),\n",
    "]\n",
    "\n",
    "# тренируем модели на всей выборке для определения важных признаков при помощи permutation_importance\n",
    "for model in models:\n",
    "    model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вызываем функцию train_permutation_importance\n",
    "common_features = train_permutation_importance(models, X, y, RAND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# разделение данных train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# признаки из permutation_importance\n",
    "X = df[list(common_features)]\n",
    "\n",
    "# целевые переменные\n",
    "y = df[['magnitude', 'longitude', 'latitude']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=RAND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# модель RandomForestRegressor\n",
    "rfr = RandomForestRegressor(random_state=RAND)\n",
    "# обучаем модель\n",
    "rfr.fit(X_train, y_train)\n",
    "#  предсказания на тестовой выборке\n",
    "y_pred_rfr = rfr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверка на переобучение модели RandomForestRegressor\n",
    "check_overfitting(rfr, X_train, y_train, X_test, y_test, mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# просмотр метрик модели RandomForestRegressor обученной на train\n",
    "metrics = get_metrics_regression(y_test,\n",
    "                                 y_pred = y_pred_rfr,\n",
    "                                 X_test = X_test,\n",
    "                                 name='RandomForestRegressor_Baseline')\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# модель DecisionTreeRegressor\n",
    "dtr = DecisionTreeRegressor(random_state=RAND)\n",
    "# обучаем модель\n",
    "dtr.fit(X_train, y_train)\n",
    "#  предсказания на тестовой выборке\n",
    "y_pred_dtr = dtr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверка на переобучение модели DecisionTreeRegressor\n",
    "check_overfitting(dtr, X_train, y_train, X_test, y_test, mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# просмотр метрик модели DecisionTreeRegressor обученной на train\n",
    "metrics = pd.concat([\n",
    "    metrics,\n",
    "    get_metrics_regression(y_test,\n",
    "                           y_pred = y_pred_dtr,\n",
    "                           X_test = X_test,\n",
    "                           name='DecisionTreeRegressor_Baseline')])\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# модель GradientBoostingRegressor\n",
    "gbr = MultiOutputRegressor(GradientBoostingRegressor(random_state=RAND))\n",
    "\n",
    "# обучаем модель\n",
    "gbr.fit(X_train, y_train)\n",
    "#  предсказания на тестовой выборке\n",
    "y_pred_gbr = gbr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверка на переобучение модели GradientBoostingRegressor\n",
    "check_overfitting(gbr, X_train, y_train, X_test, y_test, mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# просмотр метрик модели GradientBoostingRegressor обученной на train\n",
    "metrics = pd.concat([\n",
    "    metrics,\n",
    "    get_metrics_regression(y_test,\n",
    "                           y_pred = y_pred_gbr,\n",
    "                           X_test = X_test,\n",
    "                           name='GradientBoostingRegressor_Baseline')])\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# модель LightGBMRegressor\n",
    "lgbmreg = MultiOutputRegressor(LGBMRegressor(random_state=RAND, verbosity=-1))\n",
    "\n",
    "# обучаем модель\n",
    "lgbmreg.fit(X_train, y_train)\n",
    "\n",
    "# предсказания на тестовой выборке\n",
    "y_pred_lgbmreg = lgbmreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверка на переобучение модели LightGBMRegressor\n",
    "check_overfitting(lgbmreg, X_train, y_train, X_test, y_test, mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# просмотр метрик модели GradientBoostingRegressor обученной на train\n",
    "metrics = pd.concat([\n",
    "    metrics,\n",
    "    get_metrics_regression(y_test,\n",
    "                           y_pred = y_pred_lgbmreg,\n",
    "                           X_test = X_test,\n",
    "                           name='LightGBMRegressor_Baseline')])\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna & KFold подбор гиперпараметров и кросс-валидация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# целевая функция для оптимизации\n",
    "def objective(trial):\n",
    "    global best_score\n",
    "    # гиперпараметры для настройки\n",
    "    n_estimators = trial.suggest_int('n_estimators', 100, 1000)\n",
    "    max_depth = trial.suggest_int('max_depth', 5, 10)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 3)\n",
    "    ccp_alpha = trial.suggest_float('ccp_alpha', 0.01, 1)\n",
    "    min_weight_fraction_leaf = trial.suggest_float('min_weight_fraction_leaf', 0.0, 0.5)\n",
    "    max_features = trial.suggest_int('max_features', 1, 10)\n",
    "    bootstrap = trial.suggest_categorical('bootstrap', [True, False])\n",
    "\n",
    "    # RandomForestRegressor с гиперпараметрами\n",
    "    rfr = RandomForestRegressor(n_estimators=n_estimators,\n",
    "                                max_depth=max_depth,\n",
    "                                min_samples_split=min_samples_split,\n",
    "                                min_samples_leaf=min_samples_leaf,\n",
    "                                ccp_alpha = ccp_alpha,\n",
    "                                min_weight_fraction_leaf=min_weight_fraction_leaf,\n",
    "                                max_features=max_features,\n",
    "                                bootstrap=bootstrap,\n",
    "                                random_state=RAND)\n",
    "\n",
    "    # выполнение кросс-валидации\n",
    "    scores = cross_val_score(rfr, X_train, y_train, cv=KFold(n_splits=N_FOLDS, shuffle=True, random_state=RAND))\n",
    "    score = np.mean(scores)\n",
    "\n",
    "        # добавляем условие для обрезки\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "\n",
    "    if score < best_score * 0.99: # обрезаем, если score меньше 99% от лучшего score\n",
    "        raise optuna.TrialPruned()\n",
    "\n",
    "    return -score  # Optuna минимизирует целевую функцию, поэтому использую -score\n",
    "\n",
    "# проверка наличия сохраненной модели\n",
    "best_params_file = os.path.join(model_path, 'rfr_best_params.pkl')\n",
    "if os.path.exists(best_params_file):\n",
    "    print('Модель уже сохранена.')\n",
    "    with open(best_params_file, 'rb') as f:\n",
    "        rfr_best_params = pickle.load(f)\n",
    "    print('Параметры модели:', rfr_best_params.get_params())\n",
    "else:\n",
    "    print('Модель не сохранена, выполняем поиск гиперпараметров')\n",
    "    # выполнение настройки гиперпараметров с Optuna\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    best_score = float('-inf')\n",
    "    study.optimize(objective, n_trials=N_TRIALS, timeout=TIMEOUT)\n",
    "    best_params = study.best_params\n",
    "    # сохранение модели\n",
    "    rfr_best_params = RandomForestRegressor(**best_params, random_state=RAND)\n",
    "    with open(best_params_file, 'wb') as f:\n",
    "        pickle.dump(rfr_best_params, f)\n",
    "    print('Модель сохранена')\n",
    "    print('Параметры модели:', rfr_best_params.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# обучение модели\n",
    "rfr_best_params.fit(X_train, y_train)\n",
    "\n",
    "# предсказания на тестовой выборке\n",
    "y_pred_rfr_best_params = rfr_best_params.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# оценка модели\n",
    "check_overfitting(rfr_best_params, X_train, y_train, X_test, y_test, mean_squared_error)\n",
    "\n",
    "# получение метрик\n",
    "metrics = pd.concat([\n",
    "    metrics,\n",
    "    get_metrics_regression(y_test,\n",
    "                           y_pred=y_pred_rfr_best_params,\n",
    "                           X_test=X_test,\n",
    "                           name='RandomForestRegressor_best_params_optuna')])\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# целевая функция для оптимизации\n",
    "def objective(trial):\n",
    "    global best_score\n",
    "    # гиперпараметры для настройки\n",
    "    splitter = trial.suggest_categorical('splitter', ['best', 'random'])\n",
    "    max_depth = trial.suggest_int('max_depth', 5, 10)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 3)\n",
    "    ccp_alpha = trial.suggest_float('ccp_alpha', 0.01, 1)\n",
    "    min_weight_fraction_leaf = trial.suggest_float('min_weight_fraction_leaf', 0.0, 0.5)\n",
    "    max_features = trial.suggest_int('max_features', 1, 10)\n",
    "    max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 2, 5)\n",
    "\n",
    "    # DecisionTreeRegressor с гиперпараметрами\n",
    "    rfr = DecisionTreeRegressor(\n",
    "                                splitter=splitter,\n",
    "                                max_depth=max_depth,\n",
    "                                min_samples_split=min_samples_split,\n",
    "                                min_samples_leaf=min_samples_leaf,\n",
    "                                ccp_alpha = ccp_alpha,\n",
    "                                min_weight_fraction_leaf=min_weight_fraction_leaf,\n",
    "                                max_features=max_features,\n",
    "                                max_leaf_nodes=max_leaf_nodes,\n",
    "                                random_state=RAND)\n",
    "\n",
    "    # выполнение кросс-валидации\n",
    "    scores = cross_val_score(rfr, X_train, y_train, cv=KFold(n_splits=N_FOLDS, shuffle=True, random_state=RAND))\n",
    "    score = np.mean(scores)\n",
    "\n",
    "    # добавляем условие для обрезки\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "\n",
    "    if score < best_score * 0.99: # обрезаем, если score меньше 99% от лучшего score\n",
    "        raise optuna.TrialPruned()\n",
    "\n",
    "    return -score  # Optuna минимизирует целевую функцию, поэтому использую -score\n",
    "\n",
    "# проверка наличия сохраненной модели\n",
    "best_params_file = os.path.join(model_path, 'dtr_best_params.pkl')\n",
    "if os.path.exists(best_params_file):\n",
    "    print('Модель уже сохранена.')\n",
    "    with open(best_params_file, 'rb') as f:\n",
    "        dtr_best_params = pickle.load(f)\n",
    "    print('Параметры модели:', dtr_best_params.get_params())\n",
    "else:\n",
    "    print('Модель не сохранена, выполняем поиск гиперпараметров')\n",
    "    # выполнение настройки гиперпараметров с Optuna\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    best_score = float('-inf')\n",
    "    study.optimize(objective, n_trials=N_TRIALS, timeout=TIMEOUT)\n",
    "    best_params = study.best_params\n",
    "    # сохранение модели\n",
    "    dtr_best_params = DecisionTreeRegressor(**best_params, random_state=RAND)\n",
    "    with open(best_params_file, 'wb') as f:\n",
    "        pickle.dump(dtr_best_params, f)\n",
    "    print('Модель сохранена')\n",
    "    print('Параметры модели:', dtr_best_params.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# обучение модели\n",
    "dtr_best_params.fit(X_train, y_train)\n",
    "\n",
    "# предсказания на тестовой выборке\n",
    "y_pred_dtr_best_params = dtr_best_params.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# оценка модели\n",
    "check_overfitting(dtr_best_params, X_train, y_train, X_test, y_test, mean_squared_error)\n",
    "\n",
    "# получение метрик\n",
    "metrics = pd.concat([\n",
    "    metrics,\n",
    "    get_metrics_regression(y_test,\n",
    "                           y_pred=y_pred_dtr_best_params,\n",
    "                           X_test=X_test,\n",
    "                           name='DecisionTreeRegressor_best_params_optuna')])\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# целевая функция для оптимизации\n",
    "def objective(trial):\n",
    "    global best_score\n",
    "    # гиперпараметры для настройки\n",
    "    learning_rate = trial.suggest_float('estimator__learning_rate', 0.1, 1)\n",
    "    n_estimators = trial.suggest_int('estimator__n_estimators', 100, 1000)\n",
    "    subsample = trial.suggest_float('estimator__subsample', 0.1, 1.0)\n",
    "    min_samples_split = trial.suggest_int('estimator__min_samples_split', 2, 10)\n",
    "    min_samples_leaf = trial.suggest_int('estimator__min_samples_leaf', 1, 5)\n",
    "    alpha = trial.suggest_float('alpha', 0.01, 1)\n",
    "    min_weight_fraction_leaf = trial.suggest_float('estimator__min_weight_fraction_leaf', 0.0, 0.5)\n",
    "    max_depth = trial.suggest_int('estimator__max_depth', 1, 10)\n",
    "\n",
    "    # GradientBoostingRegressor с гиперпараметрами\n",
    "    gbr = MultiOutputRegressor(GradientBoostingRegressor(learning_rate=learning_rate,                                                    \n",
    "                                                         n_estimators=n_estimators,\n",
    "                                                         subsample=subsample,\n",
    "                                                         min_samples_split=min_samples_split,\n",
    "                                                         min_samples_leaf=min_samples_leaf,\n",
    "                                                         alpha = alpha,\n",
    "                                                         min_weight_fraction_leaf=min_weight_fraction_leaf,\n",
    "                                                         max_depth=max_depth,\n",
    "                                                         random_state=RAND))\n",
    "\n",
    "    # выполнение кросс-валидации\n",
    "    scores = cross_val_score(gbr, X_train, y_train, cv=KFold(n_splits=N_FOLDS, shuffle=True, random_state=RAND))\n",
    "    score = np.mean(scores)\n",
    "\n",
    "        # добавляем условие для обрезки\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "\n",
    "    if score < best_score * 0.99: # обрезаем, если score меньше 99% от лучшего score\n",
    "        raise optuna.TrialPruned()\n",
    "\n",
    "    return -score  # Optuna минимизирует целевую функцию, поэтому использую -score\n",
    "\n",
    "# проверка наличия сохраненной модели\n",
    "best_params_file = os.path.join(model_path, 'gbr_best_params.pkl')\n",
    "if os.path.exists(best_params_file):\n",
    "    print('Модель уже сохранена.')\n",
    "    with open(best_params_file, 'rb') as f:\n",
    "        gbr_best_params = pickle.load(f)\n",
    "    print('Параметры модели:', gbr_best_params.get_params())\n",
    "else:\n",
    "    print('Модель не сохранена, выполняем поиск гиперпараметров')\n",
    "    # выполнение настройки гиперпараметров с Optuna\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    best_score = float('-inf')\n",
    "    study.optimize(objective, n_trials=N_TRIALS, timeout=TIMEOUT)\n",
    "    best_params = study.best_params\n",
    "    # удалить префикс 'estimator__' из ключей\n",
    "    best_params = {k.replace('estimator__', ''): v for k, v in best_params.items()}\n",
    "    # сохранение модели\n",
    "    gbr_best_params = MultiOutputRegressor(GradientBoostingRegressor(**best_params, random_state=RAND))\n",
    "    with open(best_params_file, 'wb') as f:\n",
    "        pickle.dump(gbr_best_params, f)\n",
    "    print('Модель сохранена')\n",
    "    print('Параметры модели:', gbr_best_params.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# обучение модели\n",
    "gbr_best_params.fit(X_train, y_train)\n",
    "\n",
    "# предсказания на тестовой выборке\n",
    "y_pred_gbr_best_params = gbr_best_params.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# оценка модели\n",
    "check_overfitting(gbr_best_params, X_train, y_train, X_test, y_test, mean_squared_error)\n",
    "\n",
    "# получение метрик\n",
    "metrics = pd.concat([\n",
    "    metrics,\n",
    "    get_metrics_regression(y_test,\n",
    "                           y_pred=y_pred_gbr_best_params,\n",
    "                           X_test=X_test,\n",
    "                           name='GradientBoostingRegressor_best_params_optuna')])\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# целевая функция для оптимизации\n",
    "def objective(trial):\n",
    "    global best_score\n",
    "    # гиперпараметры для настройки\n",
    "    learning_rate = trial.suggest_float('estimator__learning_rate', 0.01, 1)\n",
    "    n_estimators = trial.suggest_int('estimator__n_estimators', 100, 1000)\n",
    "    num_leaves = trial.suggest_int('estimator__num_leaves', 31, 255)\n",
    "    subsample = trial.suggest_float('estimator__subsample', 0.1, 1.0)\n",
    "    colsample_bytree = trial.suggest_float('estimator__colsample_bytree', 0.1, 1.0)\n",
    "    reg_alpha = trial.suggest_float('estimator__reg_alpha', 0.01, 1)\n",
    "    reg_lambda = trial.suggest_float('estimator__reg_lambda', 0.01, 1)\n",
    "\n",
    "    # LGBMRegressor с гиперпараметрами\n",
    "    lgbm = MultiOutputRegressor(LGBMRegressor(learning_rate=learning_rate, \n",
    "                                               n_estimators=n_estimators, \n",
    "                                               num_leaves=num_leaves, \n",
    "                                               subsample=subsample, \n",
    "                                               colsample_bytree=colsample_bytree, \n",
    "                                               reg_alpha=reg_alpha, \n",
    "                                               reg_lambda=reg_lambda, \n",
    "                                               random_state=RAND))\n",
    "\n",
    "    # выполнение кросс-валидации\n",
    "    scores = cross_val_score(lgbmreg, X_train, y_train, cv=KFold(n_splits=N_FOLDS, shuffle=True, random_state=RAND))\n",
    "    score = np.mean(scores)\n",
    "\n",
    "    # добавляем условие для обрезки\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "\n",
    "    if score < best_score * 0.99: # обрезаем, если score меньше 99% от лучшего score\n",
    "        raise optuna.TrialPruned()\n",
    "\n",
    "    return -score  # Optuna минимизирует целевую функцию, поэтому использую -score\n",
    "\n",
    "# проверка наличия сохраненной модели\n",
    "best_params_file = os.path.join(model_path, 'lgbmreg_best_params.pkl')\n",
    "if os.path.exists(best_params_file):\n",
    "    print('Модель уже сохранена.')\n",
    "    with open(best_params_file, 'rb') as f:\n",
    "        lgbmreg_best_params = pickle.load(f)\n",
    "    print('Параметры модели:', lgbmreg_best_params.get_params())\n",
    "else:\n",
    "    print('Модель не сохранена, выполняем поиск гиперпараметров')\n",
    "    # выполнение настройки гиперпараметров с Optuna\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    best_score = float('-inf')\n",
    "    study.optimize(objective, n_trials=N_TRIALS, timeout=TIMEOUT)\n",
    "    best_params = study.best_params\n",
    "    # удалить префикс 'estimator__' из ключей\n",
    "    best_params = {k.replace('estimator__', ''): v for k, v in best_params.items()}\n",
    "    # сохранение модели\n",
    "    lgbmreg_best_params = MultiOutputRegressor(LGBMRegressor(**best_params, random_state=RAND))\n",
    "    with open(best_params_file, 'wb') as f:\n",
    "        pickle.dump(lgbmreg_best_params, f)\n",
    "    print('Модель сохранена')\n",
    "    print('Параметры модели:', lgbmreg_best_params.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# обучение модели\n",
    "lgbmreg_best_params.fit(X_train, y_train)\n",
    "\n",
    "# предсказания на тестовой выборке\n",
    "y_pred_lgbmreg_best_params = lgbmreg_best_params.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# оценка модели\n",
    "check_overfitting(lgbmreg_best_params, X_train, y_train, X_test, y_test, mean_squared_error)\n",
    "\n",
    "# получение метрик\n",
    "metrics = pd.concat([\n",
    "    metrics,\n",
    "    get_metrics_regression(y_test,\n",
    "                           y_pred=y_pred_lgbmreg_best_params,\n",
    "                           X_test=X_test,\n",
    "                           name='LightGBMRegressor_best_params_optuna')])\n",
    "metrics\n",
    "\n",
    "# сохранение metrics в файл data/metrics.csv\n",
    "metrics.to_csv('../data/metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функция выбора лучшей модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_model(X_test, y_test,\n",
    "                      rfr_best_params,\n",
    "                      dtr_best_params,\n",
    "                      gbr_best_params,\n",
    "                      lgbmreg_best_params\n",
    "                      ):\n",
    "    \"\"\"\n",
    "    Выбирает лучшую модель на основе средней квадратичной ошибки (MSE) на тестовой выборке.\n",
    "\n",
    "    Параметры:\n",
    "    - X_test: Тестовые признаки\n",
    "    - y_test: Тестовая целевая переменная\n",
    "    - rfr_best_params: Обученная модель случайного леса регрессии\n",
    "    - dtr_best_params: Обученная модель дерева решений регрессии\n",
    "    - gbr_best_params: Обученная модель градиентного бустинга регрессии\n",
    "    - lgbmreg_best_params: Обученная модель LightGBM регрессии\n",
    "\n",
    "    Возвращает:\n",
    "    - Лучшую модель на основе MSE\n",
    "    \"\"\"\n",
    "    models = [rfr_best_params,\n",
    "              dtr_best_params,\n",
    "              gbr_best_params,\n",
    "              lgbmreg_best_params,\n",
    "              ]\n",
    "    mse_values = []\n",
    "\n",
    "    for model in models:\n",
    "        y_pred = model.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mse_values.append(mse)\n",
    "\n",
    "    best_model_index = mse_values.index(min(mse_values))\n",
    "    best_model = models[best_model_index]\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# используем функцию выбора лучшей модели\n",
    "best_model = select_best_model(X_test, y_test,\n",
    "                               rfr_best_params,\n",
    "                               dtr_best_params,\n",
    "                               gbr_best_params,\n",
    "                               lgbmreg_best_params,\n",
    "                               )\n",
    "print(\"Лучшая модель:\", best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# предсказания на тестовой выборке лучшей модели\n",
    "y_pred_best_model = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y_pred_rfr_best_params = pd.DataFrame(y_pred_rfr_best_params, columns=['magnitude', 'longitude', 'latitude'])\n",
    "df_y_pred_dtr_best_params = pd.DataFrame(y_pred_dtr_best_params, columns=['magnitude', 'longitude', 'latitude'])\n",
    "df_y_pred_gbr_best_params = pd.DataFrame(y_pred_gbr_best_params, columns=['magnitude', 'longitude', 'latitude'])\n",
    "df_y_pred_lgbmreg_best_params = pd.DataFrame(y_pred_lgbmreg_best_params, columns=['magnitude', 'longitude', 'latitude'])\n",
    "df_y_pred_best_model = pd.DataFrame(y_pred_best_model, columns=['magnitude', 'longitude', 'latitude'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сравнение карт моделей и фактических данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_earthquake_maps(y_test, y_pred_model, model_name):\n",
    "    \"\"\"\n",
    "    Создает 2 карты с магнитудой реальными данными и данными из y_pred_model.\n",
    "    \n",
    "    Параметры:\n",
    "    y_test (DataFrame): Реальные данные.\n",
    "    y_pred_model (DataFrame): Предсказанные данные.\n",
    "    model_name (str): Название модели.\n",
    "    \n",
    "    Возвращает:\n",
    "    fig (Figure): \"Две карты\".\n",
    "    \"\"\"\n",
    "    y_pred_model['model_name'] = model_name\n",
    "    fig1 = px.scatter_geo(y_test, lat='latitude', lon='longitude', color='magnitude', color_continuous_scale='reds', title='Earthquakes Around the World best vs fact')\n",
    "    fig2 = px.scatter_geo(y_pred_model, lat='latitude', lon='longitude', color='magnitude', color_continuous_scale='reds', title='Earthquakes Around the World')\n",
    "    \n",
    "    for fig in [fig1, fig2]:\n",
    "        fig.update_layout(geo=dict(landcolor='white', oceancolor='lightblue', showland=True, showocean=True), width=600, height=400)\n",
    "    \n",
    "    fig = make_subplots(rows=1, cols=2, specs=[[{'type': 'geo'}, {'type': 'geo'}]], subplot_titles=['Earthquakes Around the World fact', f'Earthquakes Around the World {model_name}'])\n",
    "    fig.add_trace(fig1.data[0], row=1, col=1)\n",
    "    fig.add_trace(fig2.data[0], row=1, col=2)\n",
    "    fig.update_layout(width=1450, height=600)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Используем функцию отрисовки карты\n",
    "model_name = best_model.estimator.__class__.__name__\n",
    "fig = create_earthquake_maps(y_test, df_y_pred_best_model, model_name)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Используем функцию отрисовки карты\n",
    "model_name = rfr_best_params.__class__.__name__\n",
    "fig = create_earthquake_maps(y_test, df_y_pred_rfr_best_params, model_name)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Используем функцию отрисовки карты\n",
    "model_name = dtr_best_params.__class__.__name__\n",
    "fig = create_earthquake_maps(y_test, df_y_pred_dtr_best_params, model_name)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Используем функцию отрисовки карты\n",
    "model_name = gbr_best_params.estimator.__class__.__name__\n",
    "fig = create_earthquake_maps(y_test, df_y_pred_gbr_best_params, model_name)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Используем функцию отрисовки карты\n",
    "model_name = lgbmreg_best_params.estimator.__class__.__name__\n",
    "fig = create_earthquake_maps(y_test, df_y_pred_lgbmreg_best_params, model_name)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Обратное преобразование масштабированных данных в X_train\n",
    "# X_train[['milliseconds', 'depth', 'year', 'month', 'day', 'hour', 'minute', 'second']] = scaler.inverse_transform(X_train[['milliseconds', 'depth', 'year', 'month', 'day', 'hour', 'minute', 'second']])\n",
    "\n",
    "# # Обратное преобразование масштабированных данных в X_test\n",
    "# X_test[['milliseconds', 'depth', 'year', 'month', 'day', 'hour', 'minute', 'second']] = scaler.inverse_transform(X_test[['milliseconds', 'depth', 'year', 'month', 'day', 'hour', 'minute', 'second']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# декодирование колонки 'country'\n",
    "#X_train['country'] = le.inverse_transform(X_train['country'])\n",
    "#X_test['country'] = le.inverse_transform(X_test['country'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
